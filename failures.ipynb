{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mlica/miniconda3/envs/eye/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import mlflow\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model.BaselineEyeTrackingModel import CNN_GRU\n",
    "from model.RecurrentVisionTransformer import RVT\n",
    "from utils.training_utils import train_epoch, validate_epoch, top_k_checkpoints\n",
    "from utils.metrics import weighted_MSELoss\n",
    "from dataset import ThreeETplus_Eyetracking, ScaleLabel, NormalizeLabel, TemporalSubsample, SliceLongEventsToShort, EventSlicesToVoxelGrid, SliceByTimeEventsTargets, RandomSpatialAugmentor\n",
    "import tonic.transforms as transforms\n",
    "from tonic import SlicedDataset, DiskCachedDataset\n",
    "\n",
    "\n",
    "# Parse arguments force insert the config\n",
    "args = {\n",
    "    \"config_file\": \"./configs/rvt_2_layered_test.json\",\n",
    "    \"checkpoint\": \"mlruns/775203291142996437/81d8e4a993c94f1692b65eb1c0bcabf4/artifacts/model_best_ep265_val_loss_0.0101.pth\",\n",
    "    \"train_length\": 30,\n",
    "    \"val_length\": 30,\n",
    "    \"train_stride\": 15,\n",
    "    \"val_stride\": 30,\n",
    "    \"data_augmentation\": {\n",
    "        \"random\": {\n",
    "            \"prob_hflip\": 0.5,\n",
    "            \"rotate\": {\n",
    "                \"prob\": 0,\n",
    "                \"min_angle_deg\": 2,\n",
    "                \"max_angle_deg\": 6\n",
    "            },\n",
    "            \"zoom\": {\n",
    "                \"prob\": 0.8,\n",
    "                \"zoom_in\": {\n",
    "                    \"weight\": 8,\n",
    "                    \"factor\": {\n",
    "                        \"min\": 1,\n",
    "                        \"max\": 1.5\n",
    "                    }\n",
    "                },\n",
    "                \"zoom_out\": {\n",
    "                    \"weight\": 2,\n",
    "                    \"factor\": {\n",
    "                        \"min\": 1,\n",
    "                        \"max\": 1.2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"stream\": {\n",
    "            \"prob_hflip\": 0.5,\n",
    "            \"rotate\": {\n",
    "                \"prob\": 0,\n",
    "                \"min_angle_deg\": 2,\n",
    "                \"max_angle_deg\": 6\n",
    "            },\n",
    "            \"zoom\": {\n",
    "                \"prob\": 0.5,\n",
    "                \"zoom_out\": {\n",
    "                \"factor\": {\n",
    "                    \"min\": 1,\n",
    "                    \"max\": 1.2\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)\n",
    "\n",
    "# Load hyperparameters from JSON configuration file\n",
    "if args.config_file:\n",
    "    with open(args.config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    # Overwrite command line arguments with config file\n",
    "    for key, value in config.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "# Parameters from args (now includes config file parameters)\n",
    "factor = args.spatial_factor\n",
    "temp_subsample_factor = args.temporal_subsample_factor\n",
    "test_length = args.test_length\n",
    "data_dir = args.data_dir\n",
    "test_stride = args.test_stride\n",
    "n_time_bins = args.n_time_bins\n",
    "voxel_grid_ch_normaization = args.voxel_grid_ch_normaization\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    ScaleLabel(factor),\n",
    "    TemporalSubsample(temp_subsample_factor),\n",
    "    NormalizeLabel(pseudo_width=640*factor, pseudo_height=480*factor)\n",
    "])\n",
    "\n",
    "test_data_orig = ThreeETplus_Eyetracking(save_to=data_dir, split=\"test\",\n",
    "                transform=transforms.Downsample(spatial_factor=factor),\n",
    "                target_transform=label_transform)\n",
    "\n",
    "slicing_time_window = test_length*int(10000/temp_subsample_factor)  # microseconds\n",
    "\n",
    "test_slicer = SliceByTimeEventsTargets(slicing_time_window, overlap=0,\n",
    "                                       seq_length=test_length, seq_stride=test_stride, include_incomplete=True)\n",
    "\n",
    "post_slicer_transform = transforms.Compose([\n",
    "    SliceLongEventsToShort(time_window=int(10000/temp_subsample_factor), overlap=0, include_incomplete=True),\n",
    "    EventSlicesToVoxelGrid(sensor_size=(int(640*factor), int(480*factor), 2),\n",
    "                           n_time_bins=n_time_bins, per_channel_normalize=voxel_grid_ch_normaization)\n",
    "])\n",
    "\n",
    "test_data = SlicedDataset(test_data_orig, test_slicer, transform=post_slicer_transform)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False,\n",
    "                         num_workers=int(os.cpu_count() - 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata read from ./metadata/3et_val_vl_30_vs30_ch3/slice_metadata.h5.\n"
     ]
    }
   ],
   "source": [
    "val_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"val\", \\\n",
    "                        transform=transforms.Downsample(spatial_factor=factor),\n",
    "                        target_transform=label_transform)\n",
    "\n",
    "# Then we slice the event recordings into sub-sequences. \n",
    "# The time-window is determined by the sequence length (train_length, val_length) \n",
    "# and the temporal subsample factor.\n",
    "slicing_time_window = args.train_length*int(10000/temp_subsample_factor) #microseconds\n",
    "\n",
    "# the validation set is sliced to non-overlapping sequences\n",
    "val_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=0, \\\n",
    "                seq_length=args.val_length, seq_stride=args.val_stride, include_incomplete=False)\n",
    "\n",
    "# After slicing the raw event recordings into sub-sequences, \n",
    "# we make each subsequences into your favorite event representation, \n",
    "# in this case event voxel-grid\n",
    "post_slicer_transform = transforms.Compose([\n",
    "    SliceLongEventsToShort(time_window=int(10000/temp_subsample_factor), overlap=0, include_incomplete=True),\n",
    "    EventSlicesToVoxelGrid(sensor_size=(int(640*factor), int(480*factor), 2), \\\n",
    "                                n_time_bins=args.n_time_bins, per_channel_normalize=args.voxel_grid_ch_normaization)\n",
    "])\n",
    "\n",
    "# We use the Tonic SlicedDataset class to handle the collation of the sub-sequences into batches.\n",
    "val_data = SlicedDataset(val_data_orig, val_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}\")\n",
    "\n",
    "augmentation = RandomSpatialAugmentor(dataset_wh = (1, 1), augm_config=args.data_augmentation) \n",
    "\n",
    "# cache the dataset to disk to speed up training. The first epoch will be slow, but the following epochs will be fast.\n",
    "val_data = DiskCachedDataset(val_data, cache_path=f'./cached_dataset/val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}', transforms=augmentation)\n",
    "\n",
    "# Finally we wrap the dataset with pytorch dataloader\n",
    "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False, \\\n",
    "                                num_workers=int(os.cpu_count()-2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(args.architecture)(args).to(args.device)\n",
    "\n",
    "# load weights from a checkpoint\n",
    "if args.checkpoint:\n",
    "    model.load_state_dict(torch.load(args.checkpoint))\n",
    "else:\n",
    "    raise ValueError(\"Please provide a checkpoint file.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def plot_voxel_grid_as_rgb_to_html(voxel_grid, title, predictions=[], targets=[]):\n",
    "    voxel_grid = np.moveaxis(voxel_grid, 1, -1)  # N, C, H, W -> N, H, W, C\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    im = ax.imshow(voxel_grid[0, :, :, :])\n",
    "\n",
    "    def update(i):\n",
    "        ax.clear()  # Clear to avoid overlaying dots\n",
    "        ax.imshow(voxel_grid[i, :, :, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if len(predictions) > 0:\n",
    "            x, y = predictions[i]\n",
    "            ax.plot(x*voxel_grid.shape[2], y*voxel_grid.shape[1], 'ro')\n",
    "        if len(targets) > 0:\n",
    "            x, y = targets[i]\n",
    "            ax.plot(x*voxel_grid.shape[2], y*voxel_grid.shape[1], 'go')\n",
    "        return ax,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=range(voxel_grid.shape[0]), blit=False)\n",
    "    html_str = ani.to_jshtml()\n",
    "    plt.close(fig)\n",
    "    return html_str\n",
    "\n",
    "# Initialize HTML document\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>Animation Gallery</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Animation Gallery</h1>\n",
    "\"\"\"\n",
    "\n",
    "# Assuming val_loader is defined and properly loaded\n",
    "for i, (voxel_grid, target) in enumerate(val_loader):\n",
    "    voxel_grid = voxel_grid.to(args.device)\n",
    "    pred = model(voxel_grid).detach().cpu().numpy()[0]\n",
    "    voxel_grid_np = voxel_grid[0, :, :, :, :].cpu().numpy()\n",
    "    voxel_grid_np = (voxel_grid_np - voxel_grid_np.min()) / (voxel_grid_np.max() - voxel_grid_np.min())  # Normalize\n",
    "    html_str = plot_voxel_grid_as_rgb_to_html(voxel_grid_np, f\"Voxel grid {i}\", pred, target[0][:,:2])\n",
    "    html_doc += html_str\n",
    "\n",
    "# Close HTML document\n",
    "html_doc += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML document\n",
    "with open(\"web/animation_gallery.html\", \"w\") as f:\n",
    "    f.write(html_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
