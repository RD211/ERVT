{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on using the training pipeline for the event-based eye tracking challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd211/anaconda3/envs/event_eyetracking/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, json, os, mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model.BaselineEyeTrackingModel import CNN_GRU\n",
    "from utils.training_utils import train_epoch, validate_epoch, top_k_checkpoints\n",
    "from utils.metrics import weighted_MSELoss\n",
    "from dataset import ThreeETplus_Eyetracking, ScaleLabel, NormalizeLabel, \\\n",
    "    TemporalSubsample, NormalizeLabel, SliceLongEventsToShort, \\\n",
    "    EventSlicesToVoxelGrid, SliceByTimeEventsTargets\n",
    "import tonic.transforms as transforms\n",
    "from tonic import SlicedDataset, DiskCachedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examplar config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CHANNELS = {\n",
    "    \"T\": [32, 64, 128, 256],\n",
    "    \"S\": [48, 96, 192, 384],\n",
    "    \"B\": [64, 128, 256, 512]\n",
    "}\n",
    "\n",
    "KERNELS = [7, 3, 3, 3]\n",
    "STRIDES = [4, 2, 2, 2]\n",
    "\n",
    "\n",
    "class BlockSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, window_size):\n",
    "        super(BlockSelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.window_size = window_size # P\n",
    "        \n",
    "        self.query = nn.Linear(channels, channels)\n",
    "        self.key = nn.Linear(channels, channels)\n",
    "        self.value = nn.Linear(channels, channels)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W\n",
    "        B, C, H, W = x.size()\n",
    "        P = self.window_size\n",
    "\n",
    "        # Check if H and W are divisible by P\n",
    "        if H % P != 0 or W % P != 0:\n",
    "            raise ValueError(\"The height and width of the input must be divisible by the window size.\")\n",
    "\n",
    "        # Apply unfold on the spatial dimensions H and W\n",
    "        # The shape after unfolding will be (B, C, H/P, W/P, P, P)\n",
    "        x = x.unfold(2, P, P).unfold(3, P, P)\n",
    "        \n",
    "        # Reshape and permute to bring the window blocks to the front and merge them\n",
    "        # New shape will be (B, H/P, W/P, P*P, C)\n",
    "        x = x.contiguous().view(B, C, -1, P, P).permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        # Merge the window dimensions and flatten them\n",
    "        # New shape will be (B * H/P * W/P, P*P, C)\n",
    "        x = x.contiguous().view(B, -1, P*P, C) # Shape: (num_windows, P*P, C)\n",
    "        print(x, x.shape)\n",
    "\n",
    "        # Apply linear transformations to compute Q, K, V\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (C ** 0.5)  # Shape: (num_windows, P*P, P*P)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # Shape: (num_windows, P*P, P*P)\n",
    "\n",
    "        # Apply attention to V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # Shape: (num_windows, P*P, C)\n",
    "\n",
    "        # We reshape back to the original shape\n",
    "        attn_output = attn_output.view(B, H//P, W//P, P*P, C).permute(0, 3, 4, 1, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, C, H, W)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "class GridAttention(nn.Module):\n",
    "    def __init__(self, channels, grid_size):\n",
    "        super(GridAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.grid_size = grid_size  # G\n",
    "        \n",
    "        self.query = nn.Linear(channels, channels)\n",
    "        self.key = nn.Linear(channels, channels)\n",
    "        self.value = nn.Linear(channels, channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        G = self.grid_size\n",
    "\n",
    "        # Ensure H and W are divisible by G\n",
    "        if H % G != 0 or W % G != 0:\n",
    "            raise ValueError(\"The height and width of the input must be divisible by the grid size.\")\n",
    "\n",
    "        # Partition input into a grid\n",
    "        x = x.contiguous().view(B, C, H//G, G, W//G, G).permute(0, 3, 5, 1, 2, 4).contiguous()\n",
    "        x = x.view(B, G*G, H//G * W//G, C)\n",
    "\n",
    "        # Apply Q, K, V transformations\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.channels ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Apply attention to V\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Reshape back to the original grid structure\n",
    "        attn_output = attn_output.view(B, G, G, C, H//G, W//G).permute(0, 3, 4, 1, 5, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, C, H, W)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class RVTBlock(nn.Module):\n",
    "    def __init__(self, stage, n_time_bins = None, model_type=\"T\"):\n",
    "        super().__init__()\n",
    "        self.stage = stage\n",
    "        self.model_type = model_type\n",
    "        # If stage is not 0 then we use the previous stage channels\n",
    "        if stage == 0 and n_time_bins is None:\n",
    "            raise ValueError(\"n_time_bins must be provided for stage 0\")\n",
    "        \n",
    "        input_channels = CHANNELS[model_type][stage-1] if stage > 0 else n_time_bins\n",
    "        output_channels = CHANNELS[model_type][stage]\n",
    "        kernel_size = KERNELS[stage]\n",
    "        stride = STRIDES[stage]\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)\n",
    "        \n",
    "        # Block SA\n",
    "        self.block_sa = BlockSelfAttention(channels=output_channels, window_size=7)\n",
    "        \n",
    "        # LayerNorm\n",
    "        self.ln = nn.LayerNorm(output_channels)\n",
    "\n",
    "        # MLP\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(output_channels, output_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_channels, output_channels)\n",
    "        )\n",
    "\n",
    "        # Grid SA\n",
    "        self.grid_sa = GridAttention(channels=output_channels, grid_size=4)\n",
    "        \n",
    "        # MLP 2\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(output_channels, output_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_channels, output_channels)\n",
    "        )\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=output_channels, hidden_size=output_channels, num_layers=1, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, c, h):\n",
    "\n",
    "        print(\"Input to block SA\", x.shape)\n",
    "        # B, C, H, W\n",
    "        x_conv = self.conv(x)\n",
    "        print(\"Output of conv\", x_conv.shape)\n",
    "        x_bsa = self.block_sa(x_conv)\n",
    "        x_bsa = self.ln(x_bsa)\n",
    "        x_bsa = x_bsa + x_conv\n",
    "\n",
    "        print(\"Went through block SA\", x_bsa.shape)\n",
    "\n",
    "        x_mlp1 = self.mlp1(x_bsa)\n",
    "        x_mlp1 = x_mlp1 + x_bsa\n",
    "\n",
    "        print(\"Went through MLP1\", x_mlp1.shape)\n",
    "\n",
    "        x_gsa = self.grid_sa(x_mlp1)\n",
    "        x_gsa = x_gsa + x_mlp1\n",
    "\n",
    "        print(\"Went through grid SA\", x_gsa.shape)\n",
    "\n",
    "        x_mlp2 = self.mlp2(x_gsa)\n",
    "        x_mlp2 = x_mlp2 + x_gsa\n",
    "\n",
    "        print(\"Went through MLP2\", x_mlp2.shape)\n",
    "\n",
    "        x_lstm, (c, h) = self.lstm(x_mlp2, (c, h))\n",
    "\n",
    "        print(\"Went through LSTM\", x_lstm.shape)\n",
    "        return x_lstm, c, h\n",
    "    \n",
    "class EventBasedObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, n_time_bins, model_type='T'):\n",
    "        super().__init__()\n",
    "        self.n_time_bins = n_time_bins\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # Define the RVT stages\n",
    "        self.stages = nn.ModuleList([\n",
    "            RVTBlock(stage=i, n_time_bins=n_time_bins if i == 0 else None, model_type=model_type) \n",
    "            for i in range(4)\n",
    "        ])\n",
    "\n",
    "        # Output layer to get coordinates, assuming the output of the last LSTM has 256 channels for the 'T' model\n",
    "        self.output_layer = nn.Linear(CHANNELS[model_type][-1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        # Initialize LSTM states\n",
    "        lstm_states = [(torch.zeros(1, x.size(0), CHANNELS[self.model_type][i], device=x.device),\n",
    "                        torch.zeros(1, x.size(0), CHANNELS[self.model_type][i], device=x.device)) for i in range(4)]\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Pass the input through each of the RVT stages\n",
    "        for t in range(x.size(1)):  # iterate over timesteps in the sequence\n",
    "            print(\"Getting tensor for timestep\", t)\n",
    "            xt = x[:, t, :, :, :]  # Get the tensor for the current timestep\n",
    "            for i, stage in enumerate(self.stages):\n",
    "                print(\"Stage\", i)\n",
    "                lstm_state = lstm_states[i]\n",
    "                xt, c, h = stage(xt, *lstm_state)\n",
    "                lstm_states[i] = (c, h)  # Update LSTM states\n",
    "\n",
    "            # After the last stage, use the output to predict coordinates\n",
    "            final_output = self.output_layer(xt[:, -1, :])  # Assuming the last time step from the LSTM's output\n",
    "            outputs.append(final_output)\n",
    "\n",
    "        # Convert the list of outputs to a tensor\n",
    "        coordinates = torch.stack(outputs, dim=1)  # Shape: (batch_size, seq_len, 2)\n",
    "\n",
    "        return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'sliced_baseline.json'\n",
    "with open(os.path.join('./configs', config_file), 'r') as f:\n",
    "    config = json.load(f)\n",
    "args = argparse.Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup mlflow tracking server (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  1969090\n",
      "Metadata read from ./metadata/3et_train_tl_30_ts15_ch3/slice_metadata.h5.\n",
      "Metadata read from ./metadata/3et_val_vl_30_vs30_ch3/slice_metadata.h5.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(args.mlflow_path)\n",
    "mlflow.set_experiment(experiment_name=args.experiment_name)\n",
    "\n",
    "# Define your model, optimizer, and criterion\n",
    "model = EventBasedObjectDetectionModel(n_time_bins=3, model_type='T').to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "if args.loss == \"mse\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif args.loss == \"weighted_mse\":\n",
    "    criterion = weighted_MSELoss(weights=torch.tensor((args.sensor_width/args.sensor_height, 1)).to(args.device), \\\n",
    "                                    reduction='mean')\n",
    "else:\n",
    "    raise ValueError(\"Invalid loss name\")\n",
    "\n",
    "print(\"Model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "factor = args.spatial_factor # spatial downsample factor\n",
    "temp_subsample_factor = args.temporal_subsample_factor # downsampling original 100Hz label to 20Hz\n",
    "\n",
    "# The original labels are spatially downsampled with 'factor', downsampled to 20Hz, and normalized w.r.t width and height to [0,1]\n",
    "label_transform = transforms.Compose([\n",
    "    ScaleLabel(factor),\n",
    "    TemporalSubsample(temp_subsample_factor),\n",
    "    NormalizeLabel(pseudo_width=640*factor, pseudo_height=480*factor)\n",
    "])\n",
    "\n",
    "train_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"train\", \\\n",
    "                transform=transforms.Downsample(spatial_factor=factor), \n",
    "                target_transform=label_transform)\n",
    "val_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"val\", \\\n",
    "                transform=transforms.Downsample(spatial_factor=factor),\n",
    "                target_transform=label_transform)\n",
    "\n",
    "slicing_time_window = args.train_length*int(10000/temp_subsample_factor) #microseconds\n",
    "train_stride_time = int(10000/temp_subsample_factor*args.train_stride) #microseconds\n",
    "\n",
    "train_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=slicing_time_window-train_stride_time, \\\n",
    "                seq_length=args.train_length, seq_stride=args.train_stride, include_incomplete=False)\n",
    "# the validation set is sliced to non-overlapping sequences\n",
    "val_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=0, \\\n",
    "                seq_length=args.val_length, seq_stride=args.val_stride, include_incomplete=False)\n",
    "\n",
    "post_slicer_transform = transforms.Compose([\n",
    "    SliceLongEventsToShort(time_window=int(10000/temp_subsample_factor), overlap=0, include_incomplete=True),\n",
    "    EventSlicesToVoxelGrid(sensor_size=(int(640*factor), int(480*factor), 2), \\\n",
    "                            n_time_bins=args.n_time_bins, per_channel_normalize=args.voxel_grid_ch_normaization)\n",
    "])\n",
    "\n",
    "train_data = SlicedDataset(train_data_orig, train_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_train_tl_{args.train_length}_ts{args.train_stride}_ch{args.n_time_bins}\")\n",
    "val_data = SlicedDataset(val_data_orig, val_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}\")\n",
    "\n",
    "train_data = DiskCachedDataset(train_data, cache_path=f'./cached_dataset/train_tl_{args.train_length}_ts{args.train_stride}_ch{args.n_time_bins}')\n",
    "val_data = DiskCachedDataset(val_data, cache_path=f'./cached_dataset/val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, \\\n",
    "                            num_workers=int(os.cpu_count()-2), pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False, \\\n",
    "                        num_workers=int(os.cpu_count()-2))\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, args):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model, train_loss, metrics = train_epoch(model, train_loader, criterion, optimizer, args)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_acc_all'], step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_error_all'], step=epoch)\n",
    "\n",
    "        if args.val_interval > 0 and (epoch + 1) % args.val_interval == 0:\n",
    "            val_loss, val_metrics = validate_epoch(model, val_loader, criterion, args)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # save the new best model to MLflow artifact with 3 decimal places of validation loss in the file name\n",
    "                torch.save(model.state_dict(), os.path.join(mlflow.get_artifact_uri(), \\\n",
    "                            f\"model_best_ep{epoch}_val_loss_{val_loss:.4f}.pth\"))\n",
    "                \n",
    "                # DANGER Zone, this will delete files (checkpoints) in MLflow artifact\n",
    "                top_k_checkpoints(args, mlflow.get_artifact_uri())\n",
    "                \n",
    "            print(f\"[Validation] at Epoch {epoch+1}/{args.num_epochs}: Val Loss: {val_loss:.4f}\")\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_acc_all'], step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_error_all'], step=epoch)\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs}: Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=args.run_name):\n",
    "    # dump this training file to MLflow artifact\n",
    "    # mlflow.log_artifact(__file__) # Disabled for notebook, it is included in with the script\n",
    "\n",
    "    # Log all hyperparameters to MLflow\n",
    "    mlflow.log_params(vars(args))\n",
    "    # also dump the args to a JSON file in MLflow artifact\n",
    "    with open(os.path.join(mlflow.get_artifact_uri(), \"args.json\"), 'w') as f:\n",
    "        json.dump(vars(args), f)\n",
    "\n",
    "    # Train your model\n",
    "    model = train(model, train_loader, val_loader, criterion, optimizer, args)\n",
    "\n",
    "    # Save your model for the last epoch\n",
    "    torch.save(model.state_dict(), os.path.join(mlflow.get_artifact_uri(), f\"model_last_epoch{args.num_epochs}.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_eyetracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
